\documentclass[10pt]{article}
%\usepackage{widetext}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{array}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
%\usepackage[footnotesize,bf]{caption}
%\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[cm]{fullpage}
\usepackage{multicol}
\usepackage{authblk}




\setlength\parindent{0pt}
\setlength{\columnsep}{0.5cm}

\title{Variable population mixing in disease models}
\author[1]{Ewan R. Colman}
\begin{document}
\maketitle
\begin{multicols}{2}
\section{Introduction}
\noindent
Communicable diseases are often said to follow either frequency dependent or density dependent dynamics. Frequency dependence is found to be the case when the disease spreads through a network of social ties rather than by chance proximal encounters. Typically, outbreak sizes are not affected by density (population per unit area). Density dependence occurs when the infection can spread to anyone close enough to the host, these diseases thrive when the distances between individuals is small. The most widely accepted version of density dependence assumes that the capacity of an individual to infect others is unbounded, with effectively no time or energy restrictions and no regard for the rate at which the disease is shed. In addition to this, the mean-field models usually concern a fully-mixed population in which the frequency of contact between any two individuals is the same as any other two individuals.
\newline

In this paper, we introduce mixing to the population by considering each pair of individuals to be connected by a weighted edge. The weights vary according to a given distribution; when the variation is high the population can be thought of as having a dominant social structure, when all the weights are equal the population is well-mixed. We consider a disease model with variable infectious period and rate of shedding. We derive the relationship between the two variables (shedding rate and mixing level) and the expected disease burden ($R_{0}$). Dynamics similar to density dependence are achieved when both the rate of shedding and a the level of mixing are high.


\section{Frequency and density dependence}
Suppose we have $N$ individuals in a closed population. In the compartmental SIR model, frequency dependence is established by the presence of $N$ in the rate equation for the infected population
\begin{equation}
\frac{dI}{dt}=\beta\frac{SI}{N}
\end{equation}

The assumption here is that the population is well mixed, meaning that all individuals are equally likely to come into contact with each other. The parameter $\beta$ is an amalgamation of two independent factors, the rate of contact among individuals and the probability that a contact causes disease transmission. With density dependence it is assumed that the first of these factors, the frequency of contact between individuals, is proportional to the density $N/A$, where $A$ is the area. 

Density dependence is established by the absence of $N$ in the equivalent rate equation 
\begin{equation}
\frac{dI}{dt}=\beta SI
\end{equation}

In both cases, the parameter $\beta$ is neither a property of the disease nor is it a property of social behavior. This makes it difficult, when applying the model to data to expose the extent to which social and biological factors are driving the disease. On top of this the we are offered only two very different models and asked to choose the one which seems most likely. Should we not consider something in between?

In the next section we consider a model infectious disease coupled with a model of social behavior. We consider what density and frequency dependence mean in a population modeled as a network. We derive expressions for $R_{0}$ in a homogeneous network, i.e. all edges have the same weight, which corresponds to a well-mixed population. Then heterogeneous weight distribution is introduced with variable 

\section{The model}
We assume a population of $N$ nodes (individuals). In one time-step of length $\delta t$ we have 
\begin{center}
\begin{tabular}{c|l}
$a_{i}$& probability of contact\\
$\gamma$& Recovery rate\\
$T$ & Transmission probability
\end{tabular}
\end{center}

To model the interactions between the elements of the model we think carefully about the process that leads to an infection transmitting from one individual to another. There are two possibilities for the way this could occur that correspond loosely to the notions of frequency dependence and density dependence. In the first (FD), an event occurs usually determined by social factors, then, with some given probability this event will lead to to transmission. Examples of this are sexually transmitted diseases and the transmission of information through electronic messaging. In airborne diseases the order of events is switched, first the host will shed a payload of viral material, the virus may then be received by another individual who is in close proximity. 

To unify the two paradigms, we introduct the concept of a potentially infectious contact. A potentially infectious contact between two individuals, $i$ and $j$, is an instance when the disease in question could potentially transmit to $j$ should $i$ already be infected and $j$ not. This does not depend on $i$ actually being infected; the transmission is hypothetical. In the first case, the probability that such an interaction will occur over a duration of length $\delta t$ is 
\begin{equation}
x_{i,j}=a_{i}TP(j|i)
\end{equation}
Where $a_{i}$ is the probability that $i$ will interact, $P(j|i)$ is the conditional probability that $i$ will interact with $j$ given that $i$ interacts at all, and is the probability that the contact will lead to infection $T$. In the second case, $T$ can be though of as the probability that a viral payload will be shed (if $r>1$ then we can interpret it as the expected number of viral payloads shed), $a_{i}$ is now the probability that the individual is close enough to the population for the shed virus to reach another, and $P(j|i)$ is the probability that the viral payload will reach $j$ given that $i$ is close to the population. 

The probability that an infected $i$ transitions to the recovered state is $\gamma$. We assume that the act of transitioning to the recovered state is Markovian in the sense that the probability $\gamma$ is independent of all other events that occurred before. Consequently, the probability that $i$ remains infectious for exactly $t$ time-steps is $\gamma(1-\gamma)^{t}$. In addition, the probability that at least one potentially infectious contact occurs during the $t$ time-steps for which $i$ is infectious is $1-(1-x_{i,j})^{t}$. Thus, the probability that a secondary transmission event occurs from the infected $i$ to susceptible $j$, which considers all possible infectious periods, is 
\begin{equation}
\label{i_to_j}
\begin{split}
P(i\rightarrow j)&=\gamma\sum_{t=0}^{\infty}(1-\gamma)^{t}[1-(1-x_{i,j})^{t}]\\
&=1-\gamma\sum_{t=0}^{\infty}[(1-\gamma)(1-x_{i,j})]^{t}\\
&=1-\frac{\gamma}{1-(1-\gamma)(1-x_{i,j})}\\
&=\frac{(1-\gamma)x_{i,j}}{\gamma+(1-\gamma)x_{i,j}}.
\end{split}
\end{equation}
\subsection{Variable density dependence}
In the classical compartmental disease model, density, $N/A$ (population per unit area), is captured by the force of the equation, the right hand side of Equations \eqref{frequency_classic} and \eqref{density_classic}. The choice depends on whether the force is dependent on the the the relative proportion of infected individuals in the population or absolute number. With the interpretation we have introduced, we can define density dependence as how the number of potentially infectious contacts, $a_{i}T$, scales with the population size $N$. To unify both types of disease dynamics, and to have access to the range of intermediate types, we use
\begin{equation}
a_{i}(N)=\frac{f}{T}(N-1)^{\alpha}
\end{equation}
where $f$ is a constant. When $\alpha=0$ then the (expected) number of potentially infectious contacts that $i$ will have in one time-step is the density-independent $f$. 
\subsection{Homogeneous mixing}
The well mixed model assumes that all the contact probabilities are uniform for all node pairs, thus $P(j|i)=1/(N-1)$ for all $i$ and $j$ ($i\neq j$). The basic reproductive number is $R_{0}=(N-1)P(i\rightarrow j)$. From Eq.\eqref{i_to_j} we find  
\begin{equation}
R_{0}=\frac{(1-\gamma)f(N-1)^{\alpha}}{\gamma+(1-\gamma)f(N-1)^{\alpha-1}}
\end{equation}

%If transmission depends directly on the the number of individuals in the population then $r(N)=(N-1)r$, in this case the mean total number of interactions in one time step is $N(N-1)r/2$.
%At each time-step, we think of the system as a network containing (on average) $rN$ edges. The probability that one of these edges is between nodes $i$ and $j$ is $rN/2 \times 2/N(N-1)$ (FD) or $r$ (DD). These values are the same for all $i$ and $j$ pairs and so we the probability that any two nodes are connected in such a way, $p(\Delta I,r)$, is independent of which $i$ and $j$ are chosen. In the FD case, Eq.(\eqref{i_to_j}) becomes
%\begin{equation}
%\label{first_p}
%p(\Delta I,r)=1-\left(1-\frac{\beta r}{N-1}\right)^{\Delta I}.
%\end{equation}
%The basic reproduction number $R_{0}$ is the number of secondary infections that occur as a result of a single node becoming infected. If we assume that the entire population is susceptible this value is $R_{0}=(N-1)p(\Delta I,\beta)$, which, if we arbitrarily choose $\beta\Delta I=c$ to be constant, can be written
%\begin{equation}
%R_{0}=(N-1)\left(1-\exp\left(-\frac{cr}{N-1}\right)\right).
%\end{equation}
%As $N\rightarrow \infty$, converges to $r\beta\Delta I$. The equivalent result for the density dependent rate parameter is
%\begin{equation}
%R_{0}=(N-1)\left(1-\exp\left(-cr\right)\right).
%\end{equation}

\subsection{Heterogeneous mixing}
Heterogeneous mixing is introduced by allowing variation in the values of $x_{i,j}$. In our model we suppose that the $x_{i,j}$'s are random variables drawn from the following truncated power law distribution:

%We start by focusing on a single node, $i$, and assume that the closeness of its relationship with any other $j$ is quantified by a fixed hidden variable $x_{j}$. When $i$ chooses to interact, the probability that it will be with $j$ is $x_{j}$. The model, which we will later fit to the data, assumes that these $x$ values are unevenly distributed i.e. some relationships are stronger (higher probability) than others. Moreover, the unevenness of the distribution of relationship strengths can be varied. This variability allows us to tune the model to fit data from several real social systems. The value of the tuned parameter provides a measure of the heterogeneity in the relationship strength distribution of each one. 

\begin{equation}
\rho(x)=\frac{\phi\epsilon^{\phi}}{1-\epsilon^{\phi}}x^{-(1+\phi)} \text{ for } \epsilon<x<1
\end{equation}
There are several reasons for this choice
\begin{itemize}
\item Because it is a power-law there will be a large number of very low $x_{j}$'s and a small number of large ones. We want the largest one to be, on average, sufficiently larger than the next largest one, and the next largest, and so on. The uniform and exponential distributions do not achieve this.
 
\item There is only one variable parameter, $\gamma$, and it corresponds to the level of heterogeneity in the distribution. the larger it is, the less likely we are to generate an $x_{j}$ close to $1$.

\item If we choose $\epsilon$ correctly then the $N-1$ $x_{j}$'s will, on average, sum to $1$ (or alternatively to a value equal to the rate of activity of $i$). This is what we want since we are assuming that when $i$ interacts, it interacts with, on average, one other node. 
\end{itemize}

To find $\epsilon$ such that the last point is true consider that
\begin{equation}
\label{sum_of_x}
\mathbb{E}\left(\sum_{j \neq i}x_{j}\right) = (N-1) \langle x \rangle = f
\end{equation}
where $\langle x \rangle$ denotes the mean of $\rho(x)$ and is 
\begin{equation}
\label{x_mean}
\langle x \rangle=\frac{\phi\epsilon^{\phi}(1-\epsilon^{1-\phi})}{(1-\phi)(1-\epsilon^{\phi})}.
\end{equation}
Combining these equations we find that the only possible choice of $\epsilon$ is the solution of
\begin{equation}
\label{A_poly}
(A+1)\epsilon^{\phi}-\epsilon-A=0
\end{equation}
where
\begin{equation}
\label{A_def}
A=\frac{(1-\phi)f}{(N-1)\phi}.
\end{equation}

To calculate $R_{0}$ we now have to include the probability distribution of the randomly generated $x_{i,j}$. Since these values are not known, but the distribution is, we integrate Eq.\eqref{i_to_j} over all possible values. Thus,
\begin{equation}
\label{second_p}
\begin{split}
%p(\Delta I,r,N,\gamma)&=\int\rho(x)[1-(1-x)^{\Delta I}]dx\\
%&=1-\int\rho(x)(1-x)^{\Delta I}dx\\
%&=1-\frac{\gamma\epsilon^{\gamma}}{1-\epsilon^{\gamma}}\int_{\epsilon}^{1}x^{-(1+\gamma)}(1-x)^{\Delta I}dx\\
%&=1-\frac{\gamma\epsilon^{\gamma}}{1-\epsilon^{\gamma}}B_{1-\epsilon}(\Delta I+1,-\gamma)
R_{0}&=(N-1)\int\rho(x)\frac{(1-\gamma)x}{\gamma+(1-\gamma)x}dx\\
&=\frac{(1-\gamma)(N-1)}{\gamma}\int_{\epsilon}^{1}\frac{x^{-\phi}}{1+[(1-\gamma)/\gamma]x}dx\\
&=-(N-1)\sum_{k=0}^{\infty}\left(-\frac{1-\gamma}{\gamma}\right)^{k+1}\int_{\epsilon}^{1}x^{k-\phi}dx\\
&=-(N-1)\sum_{k=1}^{\infty}(1-\gamma^{-1})^{k}\frac{1-\epsilon^{k-\phi}}{k-\phi}
\end{split}
\end{equation}
Using Eq.\eqref{sum_of_x} we have
\begin{equation}
R_{0}=f(1-\phi^{-1})\frac{(1-\epsilon^{\phi})}{(\epsilon^{\phi}-\epsilon)}\sum_{k=1}^{\infty}(1-\gamma^{-1})^{k}\frac{1-\epsilon^{k-\phi}}{k-\phi}
\end{equation}
Note that $p$ is dependent on the density, $N$, since $\epsilon=\epsilon(N)$. As before, we are interested in $R_{0}$ and how it scales with population density. We find that when $r(N)=r$, $R_{0}$ converges, implying frequency dependence. We have
\begin{equation}
\lim_{N\rightarrow\infty}R_{0}=\lim_{N\rightarrow\infty}(N-1)p(\Delta I,r,N,\gamma)=Cr
\end{equation}
where 
\begin{equation}
C = \left\{
  \begin{array}{lr}
    \frac{(1-\gamma)}{\gamma}\left[-1-\sum_{k=0}^{\Delta I}\binom{\Delta I}{k}\frac{(-1)^{k}\gamma}{k-\gamma}\right] & \text{ if }\gamma < 1\\
    \Delta I & \text{ if }\gamma > 1
  \end{array}
\right.
\end{equation}

However, it is only when the mixing parameter $\gamma$ is small that the $R_{0}$ converges quickly; for well mixed populations that are not unrealistically huge, changes in $N$ do yield changes in $R_{0}$. 

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{lines.png}\\
\includegraphics[height=4.3cm]{heatmap.png}
\includegraphics[height=4.3cm]{phase_transition.png}
\caption{Caption goes here}
\end{figure}

\section*{Estimating $\gamma$}
The values $x_{i,j}$, and consequently their distribution $\rho$, relate to the social or spatial dynamics of the population. We can define an interaction between $i$ and $j$ as any instance when an infection can potentially transmit from one to the other. With this definition the definition of $x_{i,j}$ is equivalent to
\begin{equation}
\begin{split}
x_{i,j}&=P(\text{infection received by j}|\text{infection shed by i})\\
&=P(i\text{ interacts with }j|i\text{ interacts at all})
\end{split}
\end{equation}
It some circumstances these interactions are recorded (for example through proximity sensors or tracking data). In this section we show how the value of $\gamma$ can be estimated given that we have a record of such interactions.

Suppose we examine some data and find that node $i$ was involved in $t_{i}$ interactions. The probability that any one of those interactions is with node $j$ is $x_{i,j}$; the probability that at least one of them is with $j$ is $p(1,t_{i})$ where $p$ is given by Eq.\eqref{second_p}. We write $p(t_{i})=p(1,t_{i})$ for brevity and also to indicate that $p(t_{i})$ is independent of which $j$ we are considering. 

For a population of nodes, the data may tell us how many interactions each individual has been involved in, $t_{i}$, and with how many others they have interacted, $d_{i}$. These two variables are linked by the conditional probability $P(d_{i}|t_{i})$. Since there are $N-1$ possible nodes for $i$ to form an edge with, the probability that node $i$ will have degree $d_{i}$ is the probability of $d_{i}$ successes (i.e. an edge existing) in $N-1$ trials,
\begin{equation}
p(d_{i}|t_{i})=\binom{N-1}{d_{i}}p(t_{i})^{d_{i}}(1-p(t_{i}))^{N-1-d_{i}}.
\end{equation}
For vectors $\textbf{d}=\{d_{1},d_{2},...,d_{N}\}$ and $\textbf{t}=\{t_{1},t_{2},...,t_{N}\}$ the log-likelihood function is
\begin{equation}
\begin{split}
\log \mathcal{L}(\gamma|\textbf{d},\textbf{t})=&\\
\sum_{i=1}^{N}&\left[d_{i}\log(p(t_{i}))+(N-1-d_{i})\log(1-p(t_{i}))\right]
%=\sum_{i=1}^{N}\left[d_{i}\log()+(N-1-d_{i})\log()\right]\\
\end{split}
\end{equation}
The estimated value of the mixing parameter $\gamma$ is the value that maximizes this likelihood function.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{data_plot.png}
\caption{Caption goes here}
\end{figure}
\appendix
\section{Large $N$ limit of $R_{0}$} 
As before we define $R_{0}$ to be 
\begin{equation}
R_{0}=(N-1)p(\Delta I,r).
\end{equation}
From Eq.\eqref{A_def} we can see that as $N$ grows large, $A$ goes to $0$ and the solutions of Eq.\eqref{A_poly} are $\epsilon=0$ and $\epsilon=1$. Expressing $R_{0}$ in terms of $N$ is not possible so we instead write $R_{0}$ in terms of $\epsilon$ and find the limit as $\epsilon\rightarrow 0$. First, combining Eq.\eqref{sum_of_strengths} and Eq.\eqref{x_mean} gives
\begin{equation}
\label{N_minus_1}
N-1=\frac{(1-\gamma)r}{\gamma} \frac{(1-\epsilon^{ \gamma})}{(\epsilon^{\gamma}-\epsilon)}.
\end{equation}
For the second part, we expand $(1-x)^{\Delta I}$ in Eq.\eqref{second_p} and then perform the integration on each term to get
\begin{equation}
\label{binomial}
p(\Delta I,r)=1-\gamma\sum_{k=0}^{\Delta I}\binom{\Delta I}{k}\frac{(-1)^{k}}{k-\gamma}\frac{(\epsilon^{\gamma}-\epsilon^{k})}{1-\epsilon^{\gamma}}.
\end{equation}
Eq.\eqref{N_minus_1} and Eq.\eqref{binomial} combine to give $R_{0}$ in terms of $\epsilon$
\begin{equation}
R_{0}=\frac{(1-\gamma)r}{\gamma(\epsilon^{\gamma}-\epsilon)}\left[1-\epsilon^{\gamma}-\gamma\sum_{k=0}^{\Delta I}\binom{\Delta I}{k}\frac{(-1)^{k}}{k-\gamma}(\epsilon^{\gamma}-\epsilon^{k})\right]
\end{equation}
We cannot directly infer $\lim_{\epsilon\rightarrow 0}R_{0}$ from this equation since both the numerator and denominator go to $0$. Applying l'Hopital's rule we get
\begin{equation}
R^{\*}=\frac{(1-\gamma)r}{(\gamma\epsilon^{\gamma-1}-1)}\left[-\epsilon^{\gamma-1}-\sum_{k=0}^{\Delta I}\binom{\Delta I}{k}\frac{(-1)^{k}}{k-\gamma}(\gamma\epsilon^{\gamma-1}-k\epsilon^{k-1})\right]
\end{equation}
For $\gamma>1$ we we can find $\lim_{\epsilon\rightarrow 0}R^{\*}$ directly by substituting $\epsilon=0$,
\begin{equation}
\lim_{N\rightarrow \infty}R_{0}=r\Delta I.
\end{equation}
For $\gamma<1$, we first multiply numerator and denominator by $\epsilon^{1-\gamma}$ before substituting $\epsilon=0$. We get
\begin{equation}
\lim_{N\rightarrow \infty}R_{0}=\frac{(1-\gamma)r}{\gamma}\left[-1-\sum_{k=0}^{\Delta I}\binom{\Delta I}{k}\frac{(-1)^{k}\gamma}{k-\gamma}\right]
\end{equation}
\end{multicols}



\end{document}
